{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging\n",
    "\n",
    "The most basic and useful task while dealing with text based problems is to tokenize each word separately and label each word according to its most likely part of speech. This task is basically called as Part of Speech Tagging (POST).\n",
    "\n",
    "In this lesson, we will use the Brown Corpus, an influential dataset that is used in many studies of POST. The Brown Corpus defined a tagset (specific collection of POS labels) that has been reused in many other annotated resources in English.\n",
    "\n",
    "Recently, a different version of the tagset has been defined which is called the Universal Parts of Speech Tagset.\n",
    "\n",
    "Let’s start with NLTK installation and downloading i.e. \n",
    "```\n",
    "pip install nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "Content Courtesy: https://www.cs.bgu.ac.il/~elhadad/nlp19/nlp05.html#Using-Morphological-Clues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FreqDist NLTK Object: Counting Things\n",
    "\n",
    "FreqDist (Frequency Distribution) is an object that counts occurrences of objects. It is defined in the nltk.probability module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "list = ['a', 'b', 'a']\n",
    "fdist = FreqDist(list)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the FreqDist to explore the distribution of objects in the stream 'list':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['a']  # We saw 'a' 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['b']  # We saw 'b' 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['c']  # We saw 'c' 0 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.max()      # 'a' is the most frequent sample seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)      # we observed 2 types of objects in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.keys()     # Return the types of the objects that were observed in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.freq('a')  # 2/3 of the samples we saw were 'a' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.N()        # How many samples did we count?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the Brown Corpus with NLTK\n",
    "\n",
    "The tagged_sents version of the corpus is a list of sentences. Each sentence is a list of pairs (tuples) (word, tag). Similarly, one can access the corpus as a flat list of tagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_sents(tagset='universal')\n",
    "brown_news_words = brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s check: which word is the most common among the 100,000 words in this part of the Brown corpus? Which tag is the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdistw = FreqDist([w for (w, t) in brown_news_words])\n",
    "fdistw.N()    # We saw 1,161,192 words in this section of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdistw)  # How many distinct words are there|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdistw.max()  # What is the most frequent word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62713"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdistw['the']   # How often does 'the' occur in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5.40%\n"
     ]
    }
   ],
   "source": [
    "print('%5.2f%%' % (fdistw.freq('the') * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the distribution of word tokens to word types is extremely unbalanced - a single word (the) accounts for over 6% of the word tokens. This is a general observation of linguistic data -- known as Zipf's Law -- few types are extremely frequent, and many types are extremely rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinguishing Word Type and Work Token \n",
    "\n",
    "When distinguishing _word type_ and _word token_, we can decide to consider that strings that vary only because of case variants correspond to the same _word type_ - for example, the token _Book_ and _book_ correspond to the same word type _book_ (and so do _bOOk_ and _bOok_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us count the words without distinction of upper/lower case and the tags\n",
    "fdistwl = FreqDist([w.lower() for (w, t) in brown_news_words])\n",
    "fdistt = FreqDist([t for (w, t) in brown_news_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ignore case variants, there are only 49,815 word types instead of 62,713 when case differences are kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49815"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdistwl)  # How many distinct words when we ignore case distinctions ('the' same as 'The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.70%\n"
     ]
    }
   ],
   "source": [
    "print('%5.2f%%' % (fdistt.freq('NOUN') * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of the POST task\n",
    "\n",
    "The first question we address about the task of POS tagging is: how complex is it?\n",
    "\n",
    "One way to quantify the complexity of a task is to measure its [perplexity](http://en.wikipedia.org/wiki/Perplexity). \n",
    "The intuitive meaning of perplexity is: when the tagger is considering which tag to associate to a word, how \"confused\" is it? That is, how many options does it have to choose from?\n",
    "\n",
    "Obviously, this depends on the number of tags in the tagset. The [universal tagset](http://universaldependencies.org/u/pos/) includes 17 tags:\n",
    "\n",
    "Tag\t| Meaning\t | Examples\n",
    "----|------------|----------\n",
    "ADJ\t| adjective\t | new, good, high, special, big, local\n",
    "ADV\t| adverb\t | really, already, still, early, now\n",
    "CONJ| conjunction| and, or, but, if, while, although\n",
    "DET\t| determiner | the, a, some, most, every, no\n",
    "X\t| other, foreign words | dolce, ersatz, esprit, quo, maitre\n",
    "NOUN | noun\t     | year, home, costs, time, education\n",
    "PROPN| proper noun | Alison, Africa, April, Washington\n",
    "NUM\t | numeral\t| twenty-four, fourth, 1991, 14:24\n",
    "PRON | pronoun\t| he, their, her, its, my, I, us\n",
    "ADP  | adposition, preposition | on, of, at, with, by, into, under\n",
    "AUX\t | auxiliary verb | has (done), is (doing), will (do), should (do), must (do), can (do)\n",
    "INTJ | interjection | ah, bang, ha, whee, hmpf, oops\n",
    "VERB | verb | is, has, get, do, make, see, run\n",
    "PART | particle | possessive marker 's, negation 'not'\n",
    "SCONJ | subordinating conjunction: complementizer, adverbial clause introducer | I believe 'that' he will come, if, while\n",
    "SYM\t| symbol | $, %, (C), +, *, /, =, :), john.doe@example.com\n",
    "\n",
    "\n",
    "In the absence of any knowledge about words and tags, the perplexity of the task with a tagset of size 17 will be 17. We will see that adding knowledge will reduce the perplexity of the task.\n",
    "\n",
    "Note that the decision on how to tag a word, without more information is ambiguous for multiple reasons:\n",
    "\n",
    "- The same string can be understood as a noun or a verb (book).\n",
    "- Some POS tags have a systematically ambiguous definition: a present participle can be used in progressive verb usages (I am going:VERB), but it can also be used in an adjectival position modifying a noun: (A striking:ADJ comparison). In other words, it is unclear in the definition itself of the tag whether the tag refers to a syntactic function or to a morphological property of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring success: Accuracy, Training Dataset, Test Dataset\n",
    "\n",
    "Let’s say, we develop a tagger, how to ensure if the tagger is successful or not, whether the decision made by the tagger is good or not?\n",
    "\n",
    "The way to address these issues is to define a criterion for success, and to test the tagger on a large test dataset. Assume we have a large dataset of 1M words with their tags assigned manually. We first split the dataset into 2 parts: one part on which we will \"learn\" facts about the words and their tags (we call this the training dataset), and one part which we use to test the results of our tagger (we call this the test dataset).\n",
    "\n",
    "It is critical NOT to test our tagger on the training dataset -- because we want to test whether the tagger is able to generalize from data it has seen and make decision on unseen data. (A \"stupid\" tagger would learn the exact data seen in the training dataset \"by heart\", and respond exactly as shown when asked on training data -- it would get a perfect score on the training data, but a poor score on any unseen data.)\n",
    "\n",
    "This is one way to split the dataset into training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(brown_news_tagged)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged:  [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n",
      "Untagged:  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import brown\n",
    "# brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n",
    "# brown_news_words = brown.tagged_words(categories='news', tagset='universal')\n",
    "\n",
    "brown_train = brown_news_tagged[5000:]\n",
    "brown_test = brown_news_tagged[:5000]\n",
    "\n",
    "from nltk.tag import untag\n",
    "test_sent = untag(brown_test[0])\n",
    "print(\"Tagged: \", brown_test[0])\n",
    "print(\"Untagged: \", test_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure success, in this task, we will measure accuracy. The tagger object in NLTK includes a method called `evaluate` to measure the accuracy of a tagger on a given test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'XYZ'), ('is', 'XYZ'), ('a', 'XYZ'), ('test', 'XYZ')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A default tagger assigns the same tag to all words\n",
    "from nltk import DefaultTagger\n",
    "default_tagger = DefaultTagger('XYZ')\n",
    "default_tagger.tag('This is a test'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'NOUN' is the most frequent universal tag in the Brown corpus, we can use a tagger that assigns 'NOUN' \n",
    "to all words as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NOUN'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'NOUN'), ('Jury', 'NOUN'), ('said', 'NOUN'), ('Friday', 'NOUN'), ('an', 'NOUN'), ('investigation', 'NOUN'), ('of', 'NOUN'), (\"Atlanta's\", 'NOUN'), ('recent', 'NOUN'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'NOUN'), ('``', 'NOUN'), ('no', 'NOUN'), ('evidence', 'NOUN'), (\"''\", 'NOUN'), ('that', 'NOUN'), ('any', 'NOUN'), ('irregularities', 'NOUN'), ('took', 'NOUN'), ('place', 'NOUN'), ('.', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "default_tagger = DefaultTagger('NOUN')\n",
    "print(default_tagger.tag(test_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this baseline, we achieve a low accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.2%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %4.1f%%' % (100.0 * default_tagger.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, note that we improved the expected accuracy from picking one out of 17 answers with no knowledge, to picking one out of about 3 answers with very little knowledge (what is the most frequent tag in the dataset).\n",
    "\n",
    "The computation of the accuracy compares the answers obtained by the tagger with that listed in the \"gold standard\" dataset. This is defined as follows in the NLTK code (the izip method in Python is defined in the itertools package, given two iterable collections, it returns an iterator over a collection of pairs - iterators are \"consumed\" by code such as \"for ... in ...iterator...):\n",
    "\n",
    "```python\n",
    " def accuracy(reference, test): \n",
    "       if len(reference) != len(test): \n",
    "           raise ValueError(\"Lists must have the same length.\") \n",
    "       num_correct = 0 \n",
    "       for x, y in izip(reference, test): \n",
    "           if x == y: \n",
    "               num_correct += 1 \n",
    "       return float(num_correct) / len(reference)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of Knowledge to Improve Tagging Accuracy\n",
    "\n",
    "Intuitively, the sources of knowledge that can help us decide what is the tag of a word include:\n",
    "- A dictionary that lists the possible parts of speech for each word\n",
    "- The context of the word in a sentence (neighboring words)\n",
    "- The morphological form of the word (suffixes, prefixes)\n",
    "\n",
    "We will now develop a sequence of taggers that use these knowledge sources, and combine them. \n",
    "The taggers we develop will implement the `nltk.tag.TaggerI` interface:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup Tagger: Using Dictionary Knowledge\n",
    "\n",
    "Assume we have a dictionary that lists the possible tags for each word in English. Could we use this information to perform better tagging?\n",
    "\n",
    "The intuition is that we would only assign to a word a tag that it can have in the dictionary. For example, if \"box\" can only be a Verb or a Noun, when we have to tag an instance of the word \"box\", we only choose between 2 options - and not between 17 options. Thus, dictionary knowledge will reduce the perplexity of the task.\n",
    "\n",
    "There are 3 issues we must address to turn this into working code:\n",
    "\n",
    "- Where do we get the dictionary?\n",
    "- How do we choose between the various tags associated to a word in the dictionary? (For example, how do we choose between VERB and NOUN for \"box\").\n",
    "- What do we do for words that do not appear in the dictionary?\n",
    "\n",
    "The simple solutions we will test are the following - note that for each question, there exist other strategies that we will investigate later:\n",
    "\n",
    "- Where do we get the dictionary: we will learn it from a sample dataset.\n",
    "- How do we choose between the various tags associated to a word in the dictionary: we will choose the most likely tag as observed in the sample dataset.\n",
    "- What do we do for words that do not appear in the dictionary: we will pass unknown words to a backoff tagger.\n",
    "\n",
    "The `nltk.UnigramTagger` implements this overall strategy. It must be trained on a dataset, from which it builds a model of \"unigrams\". The following code shows how it is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", None),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'ADJ'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare training and test datasets\n",
    "# from nltk.corpus import brown\n",
    "from nltk import UnigramTagger\n",
    "\n",
    "# brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n",
    "# brown_train = brown_news_tagged[100:]\n",
    "# brown_test = brown_news_tagged[:100]\n",
    "\n",
    "# Train the unigram model\n",
    "unigram_tagger = UnigramTagger(brown_train)\n",
    "\n",
    "# Test it on a single sentence\n",
    "unigram_tagger.tag(untag(brown_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the unigram tagger leaves some words tagged as 'None' -- these are **unknown words**, words that were not observed in the training dataset.\n",
    "\n",
    "How successful is this tagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tagger accuracy: 90.5%\n"
     ]
    }
   ],
   "source": [
    "print('Unigram tagger accuracy: %4.1f%%' % ( 100.0 * unigram_tagger.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90.5% is quite an improvement on the 31% of the default tagger. \n",
    "And this is without any backoff and without using morphological clues.\n",
    "\n",
    "Is 90.5% a good level of accuracy? In fact it is not. It is accuracy per word. It means that on average, in every sentence of about 20 words, we will accumulate 2 errors. 2 errors in each sentence is a very high error rate. It makes it difficult to run another task on the output of such a tagger. Think how difficult the life of a parser would be if 2 words in every sentence are wrongly tagged. The problem is known as the **pipeline effect** -- when language processing tools are chained in a pipeline, error rates accumulate from module to module.\n",
    "\n",
    "How much would a good backoff help? Let's try first to add the NN-default tagger as a backoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tagger with backoff accuracy: 94.5%\n"
     ]
    }
   ],
   "source": [
    "nn_tagger = DefaultTagger('NOUN')\n",
    "ut2 = UnigramTagger(brown_train, backoff=nn_tagger)\n",
    "print('Unigram tagger with backoff accuracy: %4.1f%%' % ( 100.0 * ut2.evaluate(brown_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a simple backoff (with accuracy of 31%) improved accuracy from 90.5% to 94.5%. \n",
    "\n",
    "One way to report this is in terms of **error reduction**: the error rate went down from 9.5% (100-90.5) to 5.5%. \n",
    "That's an **absolute error reduction** of 9.5-5.5 = 4.0%. \n",
    "Error reduction is generally reported as a percentage of the error: 100.0 * (4.0 / 9.5) = 42.1% relative error reduction. \n",
    "\n",
    "In other words, out of the words not tagged by the original model (with no backoff), 42.1% were corrected by the backoff.\n",
    "\n",
    "What can we say about this error reduction? It is different from the accuracy of the backoff (42.1% vs 31%). \n",
    "\n",
    "One lesson to learn from this is that the **distribution of unknown words is significantly different from the distribution of all the words in the corpus**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Parts of speech are classes of words that can be characterized by criteria of different types:\n",
    "\n",
    "- Syntactic: 2 words of the same class can substitute each other in a sentence and leave the sentence syntactically acceptable\n",
    "- Morphological: words of the same class are inflected in similar manner\n",
    "- Semantic: words of the same class denote entities of similar semantic types (object, action, property, relation)\n",
    "\n",
    "\n",
    "Tagsets of various granularities can be considered. We mentioned the standard Brown corpus tagset (about 60 tags for the complete tagset) and the reduced universal tagset (17 tags).\n",
    "The key point of the approach we investigated is that it is data-driven: we attempt to solve the task by:\n",
    "- Obtain sample data annotated manually: we used the Brown corpus\n",
    "- Define a success metric: we used the definition of accuracy\n",
    "- Measure the adequacy of a method by measuring its success\n",
    "- Measure the complexity of the problem by using the notion of perplexity\n",
    "\n",
    "\n",
    "The computational methods we developed are characterized by:\n",
    "- We first define possible knowledge sources that can help us solve the task. Specifically, we investigated\n",
    "dictionary,\n",
    "morphological,\n",
    "context as possible sources.\n",
    "- We developed computational models that represent each of these knowledge sources in simple data structures (hash tables, frequency distributions, conditional frequency distributions).\n",
    "- We tested simple machine learning methods: data is acquired by inspecting a training dataset, then evaluated by testing on a test dataset.\n",
    "- We investigated one method to combine several systems into a combined system: backoff models.\n",
    "\n",
    "\n",
    "This methodology will be further developed in the next chapters, as we will address more complex tasks (parsing, summarization) and use more sophisticated machine learning methods.\n",
    "\n",
    "The task of Parts of Speech tagging is very well studied in English. The most efficient systems obtain accuracy rates of over 98% even on fine granularity tagsets - which is equivalent to the rate of success human beings obtain and the best agreement among human taggers generally obtained. The best systems use better machine learning algorithms (HMM, SVM) and treat unknown words (words not seen in training data) with more sophistication than what we have observed here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
